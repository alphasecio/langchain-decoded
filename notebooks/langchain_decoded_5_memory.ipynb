{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Decoded**"
      ],
      "metadata": {
        "id": "JQxt18IfZR40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "zgFXxUsp5pbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the LangChain package\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "fRUe2wO95xoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI package\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "T7TwICBe9yy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the API key\n",
        "import os\n",
        "\n",
        "openai_api_key = os.environ.get('OPENAI_API_KEY', 'sk-XXX')"
      ],
      "metadata": {
        "id": "I-ZUDlva6m3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Memory"
      ],
      "metadata": {
        "id": "DQ6IfBHr_nhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store and retrieve chat messages with ChatMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "history.add_user_message(\"Hello\")\n",
        "history.add_ai_message(\"Hi, how can I help you?\")\n",
        "history.add_user_message(\"I want to write Python code.\")\n",
        "history.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "history.messages"
      ],
      "metadata": {
        "id": "gYMhOgj5_x1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve chat messages with ConversationBufferHistory (as a variable)\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"Hello\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, how can I help you?\")\n",
        "memory.chat_memory.add_user_message(\"I want to write Python code.\")\n",
        "memory.chat_memory.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "RMKPUXnKpj-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve chat messages with ConversationBufferHistory (as a list of messages)\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"Hello\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, how can I help you?\")\n",
        "memory.chat_memory.add_user_message(\"I want to write Python code.\")\n",
        "memory.chat_memory.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "WCY2tsblqprw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ConversationBufferMemory in a chain\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
        "\n",
        "conversation.predict(input=\"Hello\")"
      ],
      "metadata": {
        "id": "m2Ged561sLhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "MS07RHCCtJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store a conversation summary with ConversationSummaryMemory\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.memory import ChatMessageHistory, ConversationSummaryMemory\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"Hi, how can I help you?\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "2f9H0r1XuBAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "Ne2bNr9_xNFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ConversationSummaryMemory in a chain\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(llm=llm, verbose=True, memory=memory)\n",
        "\n",
        "conversation.predict(input=\"Hello\")"
      ],
      "metadata": {
        "id": "jxmlmYBhwzPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "FW8h7QFFoLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"No, I'm a beginner.\")"
      ],
      "metadata": {
        "id": "gzl4GUCmxaEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory management using Motorhead (managed)\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.memory.motorhead_memory import MotorheadMemory\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"chat_history\", \"human_input\"], template=template)\n",
        "\n",
        "memory = MotorheadMemory(\n",
        "    api_key=\"API_KEY\",\n",
        "    client_id=\"CLIENT_ID\",\n",
        "    session_id=\"langchain-1\",\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "await memory.init();\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "llm_chain.run(\"Hello, I'm Motorhead.\")"
      ],
      "metadata": {
        "id": "WGTiwbGVYJsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(\"What's my name?\")"
      ],
      "metadata": {
        "id": "W-4D47gqg5Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory management using Motorhead (self-hosted)\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.memory.motorhead_memory import MotorheadMemory\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"chat_history\", \"human_input\"], template=template)\n",
        "\n",
        "memory = MotorheadMemory(\n",
        "    url=\"URL\",\n",
        "    session_id=\"langchain-1\",\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "await memory.init();\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "llm_chain.run(\"Hello, I'm Motorhead.\")"
      ],
      "metadata": {
        "id": "TOnq4fuXg1qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(\"What's my name?\")"
      ],
      "metadata": {
        "id": "FT0GzIbCZBeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
