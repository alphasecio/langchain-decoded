{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Decoded**"
      ],
      "metadata": {
        "id": "JQxt18IfZR40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "zgFXxUsp5pbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the LangChain package\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "fRUe2wO95xoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI package\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "T7TwICBe9yy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the API key\n",
        "import os\n",
        "\n",
        "openai_api_key = os.environ.get('OPENAI_API_KEY', 'sk-XXX')"
      ],
      "metadata": {
        "id": "I-ZUDlva6m3Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Prompts"
      ],
      "metadata": {
        "id": "U8kUt5Jm5Y6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the LLM about a recent event/occurence\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
        "\n",
        "print(llm(\"What is LangChain useful for? Answer in one sentence.\"))"
      ],
      "metadata": {
        "id": "GN9NfbApagwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the same question again, but with relevant context\n",
        "prompt = \"\"\"You are a helpful assistant, who can explain concepts in an easy-to-understand manner. Answer the following question succintly.\n",
        "          Context: There are six main areas that LangChain is designed to help with. These are, in increasing order of complexity:\n",
        "            LLMs and Prompts: This includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\n",
        "            Chains: Chains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
        "            Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\n",
        "            Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n",
        "            Memory: Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
        "            Evaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\n",
        "          Question: What is LangChain useful for?\n",
        "          Answer: \"\"\"\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "lHi9-vCAasK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a template to structure the prompt\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a helpful assistant, who is good at general knowledge trivia. Answer the following question succintly.\n",
        "              Question: {question}\n",
        "              Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "\n",
        "question = \"Who won the first football World Cup?\"\n",
        "\n",
        "print(llm(question))"
      ],
      "metadata": {
        "id": "lOwGPXanZevQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a chain to execute the prompt\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "E4e782fpa-av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save prompt template to JSON file\n",
        "prompt.save(\"myprompt.json\")\n",
        "\n",
        "# Load prompt template from JSON file\n",
        "from langchain.prompts import load_prompt\n",
        "\n",
        "saved_prompt = load_prompt(\"myprompt.json\")\n",
        "assert prompt == saved_prompt\n",
        "\n",
        "print(llm(question))"
      ],
      "metadata": {
        "id": "BGgfhiQ692V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guide the model using few shot examples in the prompt\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    { \"question\": \"How can we extend our lifespan?\", \n",
        "      \"answer\": \"Just freeze yourself and wait for technology to catch up.\"},\n",
        "    { \"question\": \"Does red wine help you live longer?\", \n",
        "      \"answer\": \"I don't know about that, but it does make the time pass more quickly.\"},\n",
        "    { \"question\": \"How can we slow down the aging process?\", \n",
        "      \"answer\": \"Simple, just stop having birthdays.\"}\n",
        "]\n",
        "\n",
        "template = \"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=template)\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=prompt,\n",
        "    prefix=\"Respond with a funny and witty remark.\",\n",
        "    suffix=\"Question: {question}\\nAnswer:\",\n",
        "    input_variables=[\"question\"],\n",
        "    example_separator=\"\"\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.format(question=\"How can I eat healthy?\"))\n",
        "print(llm(few_shot_prompt.format(question=\"How can I eat healthy?\")))"
      ],
      "metadata": {
        "id": "IzKztT8_AIiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use prompt templates with chat models\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "system_message=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
        "\n",
        "human_message=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "messages = chat_prompt.format_prompt(input_language=\"English\", output_language=\"Spanish\", text=\"I'm hungry, give me food.\").to_messages()\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "uIXTlQ4dKNL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}