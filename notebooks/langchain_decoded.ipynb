{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zgFXxUsp5pbu",
        "d_YGHcob5Rhs",
        "3MvgsUgi-yQD",
        "vpCYayVyWqyr",
        "oH043Uvlr95a",
        "U8kUt5Jm5Y6L",
        "MTkGMnWG_lyh",
        "Pc8d5uA7_4u_",
        "dIEPnFzw_7Rx",
        "yF32kJni__hY",
        "1RpFQvgAACR6",
        "m0fTtYd30V7O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Decoded**"
      ],
      "metadata": {
        "id": "JQxt18IfZR40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "zgFXxUsp5pbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the LangChain package\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "fRUe2wO95xoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI package\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "T7TwICBe9yy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the API key\n",
        "import os\n",
        "\n",
        "openai_api_key = os.environ.get('OPENAI_API_KEY', 'sk-XXX')"
      ],
      "metadata": {
        "id": "I-ZUDlva6m3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Models"
      ],
      "metadata": {
        "id": "d_YGHcob5Rhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large Language Models (LLMs)"
      ],
      "metadata": {
        "id": "3MvgsUgi-yQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the OpenAI LLM wrapper and text-davinci-003 model\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "9gqrb7Rh53uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a simple text response\n",
        "llm(\"Why is the sky blue?\")"
      ],
      "metadata": {
        "id": "1oQ3UzyEAnsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the generation output instead\n",
        "llm_result = llm.generate([\"Why is the sky blue?\"])\n",
        "llm_result.llm_output"
      ],
      "metadata": {
        "id": "wpYhYbKOC1_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Track OpenAI token usage for a single API call\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = llm(\"Why is the sky blue?\")\n",
        "\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"\\tPrompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"\\tCompletion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
      ],
      "metadata": {
        "id": "LsE4Euf2DeW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Models"
      ],
      "metadata": {
        "id": "vpCYayVyWqyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define system message for the chatbot, and pass human message\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Spanish.\"),\n",
        "    HumanMessage(content=\"Translate this sentence from English to Spanish. I'm hungry, give me food.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "Bm_9BTO1Wsz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Embeddings"
      ],
      "metadata": {
        "id": "oH043Uvlr95a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use OpenAI text embeddings for a text input\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "text = \"This is a sample query.\"\n",
        "\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(query_result)\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "id": "ozw7kkLosUqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use OpenAI text embeddings for multiple text/document inputs\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "text = [\"This is a sample query.\", \"This is another sample query.\", \"This is yet another sample query.\"]\n",
        "\n",
        "doc_result = embeddings.embed_documents(text)\n",
        "print(doc_result)\n",
        "print(len(doc_result))"
      ],
      "metadata": {
        "id": "NRH8asFsMtZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use fake embeddings to test your pipeline\n",
        "from langchain.embeddings import FakeEmbeddings\n",
        "\n",
        "embeddings = FakeEmbeddings(size=1481)\n",
        "\n",
        "text = \"This is a sample query.\"\n",
        "\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(query_result)\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "id": "-1VMjbchOZZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Request with context length > 8191 throws an error\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "long_text = 'Hello ' * 10000\n",
        "\n",
        "query_result = embeddings.embed_query(long_text)\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "9SYhZJyQTiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "t0blTkHpVOjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate input text length using tiktoken\n",
        "import tiktoken\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "max_tokens = 8191\n",
        "encoding_name = 'cl100k_base'\n",
        "\n",
        "long_text = 'Hello ' * 10000\n",
        "\n",
        "# Tokenize the input text before truncating it\n",
        "encoding = tiktoken.get_encoding(encoding_name)\n",
        "tokens = encoding.encode(long_text)[:max_tokens]\n",
        "\n",
        "# Re-convert the tokens to a string before embedding\n",
        "truncated_text = encoding.decode(tokens)\n",
        "\n",
        "query_result = embeddings.embed_query(truncated_text)\n",
        "print(query_result)\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "id": "reeAzDpqXXoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Prompts"
      ],
      "metadata": {
        "id": "U8kUt5Jm5Y6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the LLM about a recent event/occurence\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
        "\n",
        "print(llm(\"What is LangChain useful for? Answer in one sentence.\"))"
      ],
      "metadata": {
        "id": "lOwGPXanZevQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the same question again, but with relevant context\n",
        "prompt = \"\"\"You are a helpful assistant, who can explain concepts in an easy-to-understand manner. Answer the following question succintly.\n",
        "          Context: There are six main areas that LangChain is designed to help with. These are, in increasing order of complexity:\n",
        "            LLMs and Prompts: This includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\n",
        "            Chains: Chains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
        "            Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\n",
        "            Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n",
        "            Memory: Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
        "            Evaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\n",
        "          Question: What is LangChain useful for?\n",
        "          Answer: \"\"\"\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "lHi9-vCAasK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a template to structure the prompt\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a helpful assistant, who is good at general knowledge trivia. Answer the following question succintly.\n",
        "              Question: {question}\n",
        "              Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "\n",
        "question = \"Who won the first football World Cup?\"\n",
        "\n",
        "print(llm(question))"
      ],
      "metadata": {
        "id": "E4e782fpa-av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a chain to execute the prompt\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "YwlnLs8cb62N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save prompt template to JSON file\n",
        "prompt.save(\"myprompt.json\")\n",
        "\n",
        "# Load prompt template from JSON file\n",
        "from langchain.prompts import load_prompt\n",
        "\n",
        "saved_prompt = load_prompt(\"myprompt.json\")\n",
        "assert prompt == saved_prompt\n",
        "\n",
        "print(llm(question))"
      ],
      "metadata": {
        "id": "lRCAqLneb8m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guide the model using few shot examples in the prompt\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    { \"question\": \"How can we extend our lifespan?\",\n",
        "      \"answer\": \"Just freeze yourself and wait for technology to catch up.\"},\n",
        "    { \"question\": \"Does red wine help you live longer?\",\n",
        "      \"answer\": \"I don't know about that, but it does make the time pass more quickly.\"},\n",
        "    { \"question\": \"How can we slow down the aging process?\",\n",
        "      \"answer\": \"Simple, just stop having birthdays.\"}\n",
        "]\n",
        "\n",
        "template = \"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=template)\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=prompt,\n",
        "    prefix=\"Respond with a funny and witty remark.\",\n",
        "    suffix=\"Question: {question}\\nAnswer:\",\n",
        "    input_variables=[\"question\"],\n",
        "    example_separator=\"\"\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.format(question=\"How can I eat healthy?\"))\n",
        "print(llm(few_shot_prompt.format(question=\"How can I eat healthy?\")))"
      ],
      "metadata": {
        "id": "Z7tTrZJ7b-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use prompt templates with chat models\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "system_message=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
        "\n",
        "human_message=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "messages = chat_prompt.format_prompt(input_language=\"English\", output_language=\"Spanish\", text=\"I'm hungry, give me food.\").to_messages()\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "0CD1ssN8cBgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Indexes"
      ],
      "metadata": {
        "id": "MTkGMnWG_lyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Loaders"
      ],
      "metadata": {
        "id": "Pc8d5uA7_4u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured tabulate pdf2image pytesseract"
      ],
      "metadata": {
        "id": "i6zYaxTl_rlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL Loader\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "urls = [\"https://alphasec.io/summarize-text-with-langchain-and-openai\"]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "LiLWQ_tF_686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "U_uGRyko_Hbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF Loader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./data/attention-is-all-you-need.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "pages[0]"
      ],
      "metadata": {
        "id": "EdpCr38O-olp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File Directory Loader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('data', glob=\"**/*.csv\")\n",
        "docs = loader.load()\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "dJIv52X3EbyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube youtube-transcript-api"
      ],
      "metadata": {
        "id": "080GK3Q3Iv5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube Transcripts Loader\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=yEgHrxvLsz0\", add_video_info=True)\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "B13lagzPGn1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "Hwab0v2IbBYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Cloud Storage File Loader\n",
        "from langchain.document_loaders import GCSFileLoader\n",
        "\n",
        "loader = GCSFileLoader(project_name=\"langchain-gcs\", bucket=\"langchain-gcs\", blob=\"lorem-ipsum.txt\")\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "WO7Vsn_4bB7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Splitters"
      ],
      "metadata": {
        "id": "dIEPnFzw_7Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "text = uploaded[filename].decode(\"utf-8\")\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([text])\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[2])"
      ],
      "metadata": {
        "id": "WGb0P3fS__BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive Character Text Splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "text = uploaded[filename].decode(\"utf-8\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([text])\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[2])"
      ],
      "metadata": {
        "id": "NmYW0cN0UhxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Stores"
      ],
      "metadata": {
        "id": "yF32kJni__hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb tiktoken"
      ],
      "metadata": {
        "id": "rMtIR-tRX0Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chroma Vector Store\n",
        "import os, tiktoken\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = '' # @param {type:\"string\"}\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "loader = TextLoader(filename)\n",
        "data = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What comes after 'Vestibulum congue convallis finibus'?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "0XUsuto_ABze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrievers"
      ],
      "metadata": {
        "id": "1RpFQvgAACR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf"
      ],
      "metadata": {
        "id": "ZYC12MjFPQTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arxiv Retriever\n",
        "from langchain.retrievers import ArxivRetriever\n",
        "\n",
        "retriever = ArxivRetriever(load_max_docs=2)\n",
        "docs = retriever.get_relevant_documents(query='2203.15556')\n",
        "\n",
        "docs[0].metadata"
      ],
      "metadata": {
        "id": "mwVrbBUfAEaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "26WMZAxdTEzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia Retriever\n",
        "from langchain.retrievers import WikipediaRetriever\n",
        "\n",
        "retriever = WikipediaRetriever()\n",
        "docs = retriever.get_relevant_documents(query='large language models')\n",
        "\n",
        "docs[0].metadata"
      ],
      "metadata": {
        "id": "-pY7LCZ8TG-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb tiktoken"
      ],
      "metadata": {
        "id": "8LrQSBVaUFmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chroma Vector Store Retriever\n",
        "import os, tiktoken\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = '' # @param {type:\"string\"}\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "loader = TextLoader(filename)\n",
        "data = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "query = \"What comes after 'Vestibulum congue convallis finibus'?\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "S0ZiBwNKSaqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Memory"
      ],
      "metadata": {
        "id": "m0fTtYd30V7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store and retrieve chat messages with ChatMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "history.add_user_message(\"Hello\")\n",
        "history.add_ai_message(\"Hi, how can I help you?\")\n",
        "history.add_user_message(\"I want to write Python code.\")\n",
        "history.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "history.messages"
      ],
      "metadata": {
        "id": "MeseqbKK0V7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve chat messages with ConversationBufferHistory (as a variable)\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"Hello\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, how can I help you?\")\n",
        "memory.chat_memory.add_user_message(\"I want to write Python code.\")\n",
        "memory.chat_memory.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "RMKPUXnKpj-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve chat messages with ConversationBufferHistory (as a list of messages)\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"Hello\")\n",
        "memory.chat_memory.add_ai_message(\"Hi, how can I help you?\")\n",
        "memory.chat_memory.add_user_message(\"I want to write Python code.\")\n",
        "memory.chat_memory.add_ai_message(\"Sure, I can help with that. What do you want to code?\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "WCY2tsblqprw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ConversationBufferMemory in a chain\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
        "\n",
        "conversation.predict(input=\"Hello\")"
      ],
      "metadata": {
        "id": "m2Ged561sLhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "MS07RHCCtJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store a conversation summary with ConversationSummaryMemory\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.memory import ChatMessageHistory, ConversationSummaryMemory\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"Hi, how can I help you?\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "2f9H0r1XuBAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "Ne2bNr9_xNFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ConversationSummaryMemory in a chain\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(llm=llm, verbose=True, memory=memory)\n",
        "\n",
        "conversation.predict(input=\"Hello\")"
      ],
      "metadata": {
        "id": "jxmlmYBhwzPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I want to write Python code.\")"
      ],
      "metadata": {
        "id": "FW8h7QFFoLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"No, I'm a beginner.\")"
      ],
      "metadata": {
        "id": "gzl4GUCmxaEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Memory management using Motorhead (managed)\n",
        "from langchain.memory.motorhead_memory import MotorheadMemory\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"chat_history\", \"human_input\"], template=template)\n",
        "\n",
        "memory = MotorheadMemory(\n",
        "    api_key=\"API_KEY\",\n",
        "    client_id=\"CLIENT_ID\",\n",
        "    session_id=\"langchain-1\",\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "await memory.init();\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "llm_chain.run(\"Hello, I'm Motorhead.\")"
      ],
      "metadata": {
        "id": "WGTiwbGVYJsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(\"What's my name?\")"
      ],
      "metadata": {
        "id": "FT0GzIbCZBeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory management using Motorhead (self-hosted)\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.memory.motorhead_memory import MotorheadMemory\n",
        "\n",
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"chat_history\", \"human_input\"], template=template)\n",
        "\n",
        "memory = MotorheadMemory(\n",
        "    url=\"URL\",\n",
        "    session_id=\"langchain-1\",\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "await memory.init();\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
        "\n",
        "llm_chain.run(\"Hello, I'm Motorhead.\")"
      ],
      "metadata": {
        "id": "TOnq4fuXg1qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(\"What's my name?\")"
      ],
      "metadata": {
        "id": "Po6Ce7HRipd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}